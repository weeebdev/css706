Task 1: Comparative Analysis of Optimization Algorithms
1. Choose a Complex Dataset:
Each student should select a complex dataset (e.g., CIFAR-10 for image classification or a graph dataset for GNN tasks like node classification).

2. Apply Different Optimizers:
Implement and compare at least three different optimization algorithms (SGD, Adam, RMSProp, etc.) on the selected dataset. Students should analyze how each optimizer affects training speed, convergence, and final accuracy.

3. Performance Metrics:
You should track key metrics, such as loss and accuracy curves, training time, and memory usage, and create a comparison report.

Task 2: Mitigating Vanishing Gradient and Local Optima

1. Experiment with Activation Functions:
Test out different activation functions (ReLU, Leaky ReLU, Sigmoid, Tanh) in deep networks (both GNN and standard feedforward networks). You should demonstrate how activation choice affects the vanishing gradient issue.
2. Use Techniques to Avoid Local Optima:
Implement strategies like learning rate scheduling, momentum, or gradient clipping. You need to analyze how these methods affect escaping local optima.

Task 3: Report and Reflection

1. Write a Report:
The report should cover the following:
• A comparison of the performance of different optimizers.
• Insights gained from tweaking GNN architectures.
• The impact of activation functions on vanishing gradient problems.
• Strategies used to avoid local optima and their effectiveness.
2. Code Submission:
You need to submit well-commented code along with visualizations of their results (e.g., training curves, accuracy charts, gradient plots).