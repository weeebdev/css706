---
theme: seriph
background: https://images.unsplash.com/photo-1620712943543-bcc4688e7485?w=1920
class: text-center
highlighter: shiki
lineNumbers: false
info: |
  ## Project 2: Reinforcement Learning
  Training RL agents for Drone Hover (Gym-PyBullet-Drones)
  **Authors:** Adil Akhmetov, Perizat Yessenova
drawings:
  persist: false
transition: slide-left
title: Reinforcement Learning - Drone Hover
mdc: true
css:
  - ./styles.css
---

# Reinforcement Learning
## Training Agents for Drone Hover (HoverAviary)

<div class="mt-4 text-sm opacity-75">
  By Adil Akhmetov &amp; Perizat Yessenova
</div>

<div class="pt-12">
  <span class="px-2 py-1 rounded cursor-pointer" hover="bg-white bg-opacity-10">
    Project 2 - Stable Baselines3 Implementation (Full Points)
  </span>
</div>

<div class="abs-br m-6 flex gap-2">
  <span class="text-sm opacity-50">SAC ‚Ä¢ DDPG ‚Ä¢ PPO ‚Ä¢ TD3</span>
</div>

---
transition: fade-out
---

# Project Overview

<div class="grid grid-cols-2 gap-8">

<div>

## Objectives

- ‚úÖ Implement **4 RL algorithms** (SAC, DDPG, PPO, TD3)
- ‚úÖ Train on **Gym-PyBullet-Drones (HoverAviary)** for full points
- ‚úÖ Provide **unfolded algorithms** with pseudocode
- ‚úÖ Create **visualizations** and graphical results
- ‚úÖ Compare algorithm performance

</div>

<div>

## Tools & Libraries

- **Gym-PyBullet-Drones** - Drone environments (HoverAviary)
- **Stable Baselines3** - RL algorithms
- **PyTorch** - Deep learning backend
- **Matplotlib/Seaborn** - Visualization
- **TensorBoard** - Training monitoring

</div>

</div>

<div class="mt-8 p-4 bg-green-500 bg-opacity-20 rounded-lg">
  <strong>Hardware:</strong> 2√ó NVIDIA GeForce RTX 5090 GPUs
</div>

---
layout: two-cols
---

# Environment: HoverAviary (Gym-PyBullet-Drones)

<div class="pr-4">

## Description

Single quadrotor hovering at target [0, 0, 1] m.

## Observation Space
- Kinematic state (position, velocity, orientation, angular rates)

## Action Space
- **4 continuous actions**: motor RPMs
- Clipped to safe RPM ranges

## Reward
- Penalize distance from target hover, encourage stability

</div>

:::right::

<div class="pl-4">

```python
from gym_pybullet_drones.envs.HoverAviary import HoverAviary
from gym_pybullet_drones.utils.enums import DroneModel, Physics, ActionType, ObservationType

env = HoverAviary(
    drone_model=DroneModel.CF2X,
    physics=Physics.PYB,
    obs=ObservationType.KIN,
    act=ActionType.RPM,
    gui=False
)
```

<div class="mt-4 text-center">
  <strong class="text-green-400">Full-points environment: Gym-PyBullet-Drones</strong>
</div>

</div>

---
layout: two-cols
---

# Unfolded Algorithms (Pseudocode)

## Actor-Critic Core (SAC / TD3 / DDPG)
```text
Initialize actor œÄŒ∏, critics QœÜ1,QœÜ2, target networks
Replay buffer D
for each step:
  observe s, sample a ~ œÄŒ∏(s)+noise, step env -> (s', r, done)
  store (s,a,r,s',done) in D
  sample batch B from D
  y = r + Œ≥ * (1-done) * min_i QœÜi_target(s', œÄŒ∏_target(s'))
  Update critics to minimize (QœÜi(s,a) - y)^2
  if step % policy_delay == 0:
    Update actor to maximize QœÜ1(s, œÄŒ∏(s))
    Soft-update targets: Œ∏_target ‚Üê œÑŒ∏ + (1-œÑ)Œ∏_target; same for œÜi
```

## PPO (Clipped Objective)
```text
Collect trajectories with œÄŒ∏_old for T steps
Compute advantages AÃÇ via GAE
for K epochs:
  L_clip = E[min(r_t(Œ∏) AÃÇ_t, clip(r_t(Œ∏),1-Œµ,1+Œµ) AÃÇ_t)]
  Update Œ∏ to maximize L_clip
  Value/networks updated with MSE to returns
r_t(Œ∏) = œÄŒ∏(a_t|s_t) / œÄŒ∏_old(a_t|s_t)
Clip keeps updates stable
```

## DDPG vs TD3 vs SAC
- DDPG: single critic, deterministic actor, OU/GA noise.
- TD3: twin critics, delayed actor update, target policy smoothing.
- SAC: twin critics, stochastic actor, entropy term Œ±¬∑H(œÄ) encourages exploration.

---
layout: center
class: text-center
---

# Training Configuration (Drone Hover)

<div class="grid grid-cols-2 gap-8 mt-4">

<div>

## Hyperparameters (tuned)

| Parameter | SAC | DDPG | PPO | TD3 |
|-----------|-----|------|-----|-----|
| Learning Rate | 3e-4 | 5e-4 | 2.5e-4 | 5e-4 |
| Buffer Size | 200k | 200k | - | 200k |
| Batch Size | 256 | 256 | 256 | 256 |
| Gamma (Œ≥) | 0.99 | 0.99 | 0.995 | 0.99 |
| Tau (œÑ) | 0.005 | 0.005 | - | 0.005 |
| Timesteps | 200k | 200k | 200k | 200k |

</div>

<div>

## Training Setup

```python
TOTAL_TIMESTEPS = 200_000
ALGORITHMS = ['SAC','DDPG','PPO','TD3']
# HoverAviary, ActionType.RPM, ObservationType.KIN
```

</div>

</div>

---

# Training Results (HoverAviary)

<div class="grid grid-cols-2 gap-8">

<div>

## Training Time

| Algorithm | Time (s) |
|-----------|----------|
| **PPO** ‚ö° | **361.8** |
| DDPG | 868.4 |
| TD3 | 963.6 |
| SAC | 2518.2 |

<div class="mt-4 p-3 bg-blue-500 bg-opacity-20 rounded">
  PPO trains ~7√ó faster than SAC.
</div>

</div>

<div>

## Evaluation (10 episodes)

| Algorithm | Mean Reward | Std Dev |
|-----------|-------------|---------|
| **DDPG** üèÜ | **464.67** | 0.00 |
| PPO | 141.97 | 75.56 |
| SAC | 18.00 | 0.00 |
| TD3 | 16.00 | 0.00 |

<div class="mt-4 p-3 bg-green-500 bg-opacity-20 rounded">
  DDPG achieved the best hover reward.
</div>

</div>

</div>

---
layout: image-right
image: drone_training_results.png
---

# Learning Curves (Hover)

## Observations

1. **PPO** improves quickest; DDPG improves steadily.
2. **SAC/TD3** show modest gains at 200k steps.
3. Off-policy methods benefit from larger buffers and more steps.

## Key Insights

- 200k steps markedly improved hover stability.
- More steps (300k‚Äì500k) likely to further lift rewards.
- Reward smoothing helps identify steady hover.

---
layout: image-right
image: drone_evaluation_results.png
---

# Evaluation Comparison

## Best Performer: **DDPG**
- Mean Reward: **464.67**
- Std: 0.00

## Fastest: **PPO**
- Training time: **361.8s**

## Notes
- PPO shows higher variance (std 75.56) but good gains.
- SAC/TD3 remained low at 200k steps; need more training or reward shaping.

---
layout: image-right
image: drone_behavior.png
---

# Drone Behavior

## DDPG Trajectory

- Tracks target hover (0,0,1 m)
- Height plot close to target line
- Rewards stabilized across steps

## Observations
- Stable hover with deterministic policy
- Further reward shaping could speed convergence

---

# Key Findings (Drone Hover)

<div class="grid grid-cols-3 gap-6">

<div class="p-4 bg-green-500 bg-opacity-20 rounded-lg text-center">
  <div class="text-4xl mb-2">üèÜ</div>
  <h3 class="font-bold">Best Performer</h3>
  <p class="text-2xl text-green-400">DDPG</p>
  <p class="text-sm opacity-75">Reward: 464.67</p>
  <p class="text-sm opacity-75">Std: 0.00</p>
</div>

<div class="p-4 bg-blue-500 bg-opacity-20 rounded-lg text-center">
  <div class="text-4xl mb-2">‚ö°</div>
  <h3 class="font-bold">Fastest Training</h3>
  <p class="text-2xl text-blue-400">PPO</p>
  <p class="text-sm opacity-75">361.8 seconds</p>
</div>

<div class="p-4 bg-purple-500 bg-opacity-20 rounded-lg text-center">
  <div class="text-4xl mb-2">üéØ</div>
  <h3 class="font-bold">Most Stable</h3>
  <p class="text-2xl text-purple-400">SAC</p>
  <p class="text-sm opacity-75">Std: 0.00 (eval)</p>
</div>

</div>

<div class="mt-8 p-4 bg-yellow-500 bg-opacity-20 rounded-lg">

### Algorithm Comparison Summary (HoverAviary)

| Metric | SAC | DDPG | PPO | TD3 |
|--------|-----|------|-----|-----|
| Final Train Reward | 15.87 | 464.67 | 76.38 | 18.00 |
| Eval Reward | 18.00 | **464.67** | 141.97 | 16.00 |
| Training Time | 2518.2s | 868.4s | **361.8s** | 963.6s |
| Std (Eval) | 0.00 | 0.00 | 75.56 | 0.00 |

</div>

---

# Recommendations (Drone Hover)

<div class="grid grid-cols-2 gap-8">

<div>

## For This Task (HoverAviary)

1. **Use DDPG** for highest hover reward at 200k steps.
2. **PPO** is fastest; extend to 300k‚Äì500k to reduce variance.
3. **Increase steps** to 300k‚Äì500k for SAC/TD3 to catch up.

4. **Hyperparameter tuning**
   - Learning rate: 2e-4 to 5e-4
   - Batch size: 256
   - Buffer: 200k‚Äì500k
   - Œ≥: 0.99‚Äì0.995

</div>

<div>

## Future Improvements
- Reward shaping (penalize drift from hover, smooth control)
- Parallel environments (n_envs > 1)
- Domain randomization (wind, mass) for robustness
- Longer training (500k) for SAC/TD3 stability gains

</div>

</div>

---
layout: center
class: text-center
---

# Conclusion

<div class="text-xl mt-8">

‚úÖ Implemented **4 RL algorithms**: SAC, DDPG, PPO, TD3  
‚úÖ Trained on **Gym-PyBullet-Drones HoverAviary** (full points)  
‚úÖ Provided **unfolded algorithms** with detailed pseudocode  
‚úÖ Created **visualizations**: learning curves, comparisons, drone trajectories  
‚úÖ **DDPG** achieved the best hover reward (464.67)  
‚úÖ **PPO** delivered fastest training (361.8s)

</div>

<div class="mt-12 p-6 bg-green-500 bg-opacity-20 rounded-lg inline-block">
  <strong>Project Requirements: Fully Satisfied ‚úì (Full Points)</strong>
</div>

---
layout: center
class: text-center
---

# Thank You!

<div class="text-lg opacity-75 mt-4">
  Project 2 - Drone Hover Control (Gym-PyBullet-Drones)
</div>

<div class="mt-8 grid grid-cols-4 gap-4 text-sm">
  <div class="p-2 bg-white bg-opacity-10 rounded">
    <strong>SAC</strong>
    <br>2518.2s
  </div>
  <div class="p-2 bg-white bg-opacity-10 rounded">
    <strong>DDPG üèÜ</strong>
    <br>868.4s
  </div>
  <div class="p-2 bg-green-500 bg-opacity-30 rounded">
    <strong>PPO ‚ö°</strong>
    <br>361.8s
  </div>
  <div class="p-2 bg-white bg-opacity-10 rounded">
    <strong>TD3</strong>
    <br>963.6s
  </div>
</div>

<div class="abs-br m-6 text-sm opacity-50">
  Powered by Stable Baselines3, PyTorch, and Gym-PyBullet-Drones
</div>

---
layout: end
---

# Questions?

All code and results are available in the drone notebook:

`p1/drone_training.ipynb`

