{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9df58f31"
      },
      "source": [
        "# Task\n",
        "Set up and simulate a 3x3 grid world Markov Decision Process (MDP), including defining states, actions, rewards, and transition probabilities. Implement and evaluate a simple greedy policy for this MDP, and summarize the findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75db7e3e"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "### Subtask:\n",
        "Install the necessary Python libraries: `gymnasium` (or `gym`) for environment simulation and `numpy` for matrix operations. This ensures we have the tools required to build and interact with the MDP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76b539b5"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires installing `gymnasium` and `numpy` libraries. I will use `pip install` in a code block to install them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "489c9a48",
        "outputId": "08db5078-af59-47b0-a7a1-ab6d8160f2b5"
      },
      "source": [
        "pip install gymnasium numpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "317ce971"
      },
      "source": [
        "## Define MDP Grid World\n",
        "\n",
        "### Subtask:\n",
        "Define the 3x3 grid world scenario. This includes defining the states (9 cells), actions (Up, Down, Left, Right), rewards (+10 for goal, -5 for danger, 0 otherwise), and the discount factor (0.9). We will also identify the goal and danger zones within the grid.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "621fc891"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the grid dimensions, map states, define actions, create the reward structure, identify goal and danger states, and set the discount factor in a single Python code block, as outlined in the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "436f285b",
        "outputId": "1f53f760-908d-4597-cf2f-e935d348492c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Define grid dimensions\n",
        "GRID_SIZE = (3, 3)\n",
        "NUM_STATES = GRID_SIZE[0] * GRID_SIZE[1]\n",
        "\n",
        "# 2. Map cells to state identifiers (0 to 8 for a 3x3 grid)\n",
        "# We can represent states as a single integer from 0 to 8, or as (row, col) tuples.\n",
        "# For simplicity, we'll use single integers and convert to (row, col) when needed.\n",
        "\n",
        "def state_to_coords(state):\n",
        "    return (state // GRID_SIZE[1], state % GRID_SIZE[1])\n",
        "\n",
        "def coords_to_state(row, col):\n",
        "    return row * GRID_SIZE[1] + col\n",
        "\n",
        "print(f\"Number of states: {NUM_STATES}\")\n",
        "\n",
        "# 3. Define the set of possible actions\n",
        "ACTIONS = {'UP': 0, 'DOWN': 1, 'LEFT': 2, 'RIGHT': 3}\n",
        "ACTIONS_NAMES = {0: 'UP', 1: 'DOWN', 2: 'LEFT', 3: 'RIGHT'}\n",
        "NUM_ACTIONS = len(ACTIONS)\n",
        "\n",
        "print(f\"Available actions: {list(ACTIONS.keys())}\")\n",
        "\n",
        "# 4. Create a reward matrix or dictionary\n",
        "# Let's define specific goal and danger zones first, then build the reward structure.\n",
        "\n",
        "# 5. Identify the coordinates or state indices for the goal cell and any danger cells\n",
        "GOAL_COORD = (0, 2)  # Top-right corner\n",
        "DANGER_COORDS = [(1, 1)] # Center cell as danger\n",
        "\n",
        "GOAL_STATE = coords_to_state(*GOAL_COORD)\n",
        "DANGER_STATES = [coords_to_state(*coord) for coord in DANGER_COORDS]\n",
        "\n",
        "# Initialize rewards for all states to 0\n",
        "REWARDS = np.zeros(NUM_STATES)\n",
        "\n",
        "# Set rewards for goal and danger states\n",
        "REWARDS[GOAL_STATE] = 10\n",
        "for d_state in DANGER_STATES:\n",
        "    REWARDS[d_state] = -5\n",
        "\n",
        "print(f\"Goal state (coords: {GOAL_COORD}): {GOAL_STATE}, Reward: {REWARDS[GOAL_STATE]}\")\n",
        "print(f\"Danger states (coords: {[state_to_coords(s) for s in DANGER_STATES]}): {DANGER_STATES}, Rewards: {REWARDS[DANGER_STATES]}\")\n",
        "print(\"Reward structure defined.\")\n",
        "\n",
        "# 6. Set the discount factor\n",
        "DISCOUNT_FACTOR = 0.9\n",
        "\n",
        "print(f\"Discount factor (gamma): {DISCOUNT_FACTOR}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of states: 9\n",
            "Available actions: ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
            "Goal state (coords: (0, 2)): 2, Reward: 10.0\n",
            "Danger states (coords: [(1, 1)]): [4], Rewards: [-5.]\n",
            "Reward structure defined.\n",
            "Discount factor (gamma): 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19ea194e"
      },
      "source": [
        "## Implement Transition Dynamics and Reward Function\n",
        "\n",
        "### Subtask:\n",
        "Write Python functions to model the MDP's core logic. One function will determine the next state given a current state and action, incorporating the 80% intended movement and 20% random movement transition probabilities. Another function will calculate the reward for transitioning to a specific state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "253afd17"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing two Python functions: `get_next_state` to determine the probabilistic next states based on an action and `get_reward` to retrieve the reward for a given state. I will define these functions in a code block, incorporating boundary conditions and the specified transition probabilities for `get_next_state` and a direct lookup for `get_reward`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ad188ca",
        "outputId": "f446e3ee-23bd-45bb-8e49-1c29da5b373f"
      },
      "source": [
        "import random\n",
        "\n",
        "# Helper function to get valid next coordinates, handling grid boundaries\n",
        "def _get_valid_next_coords(row, col, d_row, d_col):\n",
        "    new_row, new_col = row + d_row, col + d_col\n",
        "    if 0 <= new_row < GRID_SIZE[0] and 0 <= new_col < GRID_SIZE[1]:\n",
        "        return new_row, new_col\n",
        "    else:\n",
        "        # If out of bounds, agent stays in the current cell\n",
        "        return row, col\n",
        "\n",
        "def get_next_state(state, action_idx):\n",
        "    \"\"\"\n",
        "    Determines the probabilistic next states given a current state and an action.\n",
        "    Transition probabilities: 80% intended, 10% for each perpendicular action.\n",
        "    \"\"\"\n",
        "    current_row, current_col = state_to_coords(state)\n",
        "    possible_transitions = []\n",
        "\n",
        "    # Define movement offsets for actions\n",
        "    move_offsets = {\n",
        "        ACTIONS['UP']: (-1, 0),\n",
        "        ACTIONS['DOWN']: (1, 0),\n",
        "        ACTIONS['LEFT']: (0, -1),\n",
        "        ACTIONS['RIGHT']: (0, 1)\n",
        "    }\n",
        "\n",
        "    # Define perpendicular actions for each main action\n",
        "    perpendicular_actions = {\n",
        "        ACTIONS['UP']: [ACTIONS['LEFT'], ACTIONS['RIGHT']],\n",
        "        ACTIONS['DOWN']: [ACTIONS['LEFT'], ACTIONS['RIGHT']],\n",
        "        ACTIONS['LEFT']: [ACTIONS['UP'], ACTIONS['DOWN']],\n",
        "        ACTIONS['RIGHT']: [ACTIONS['UP'], ACTIONS['DOWN']]\n",
        "    }\n",
        "\n",
        "    # 1. Intended move (80% probability)\n",
        "    dr_intended, dc_intended = move_offsets[action_idx]\n",
        "    next_row_intended, next_col_intended = _get_valid_next_coords(current_row, current_col, dr_intended, dc_intended)\n",
        "    next_state_intended = coords_to_state(next_row_intended, next_col_intended)\n",
        "    possible_transitions.append((0.8, next_state_intended))\n",
        "\n",
        "    # 2. Perpendicular moves (10% each)\n",
        "    perp_action_1, perp_action_2 = perpendicular_actions[action_idx]\n",
        "\n",
        "    # Perpendicular move 1\n",
        "    dr_perp1, dc_perp1 = move_offsets[perp_action_1]\n",
        "    next_row_perp1, next_col_perp1 = _get_valid_next_coords(current_row, current_col, dr_perp1, dc_perp1)\n",
        "    next_state_perp1 = coords_to_state(next_row_perp1, next_col_perp1)\n",
        "    possible_transitions.append((0.1, next_state_perp1))\n",
        "\n",
        "    # Perpendicular move 2\n",
        "    dr_perp2, dc_perp2 = move_offsets[perp_action_2]\n",
        "    next_row_perp2, next_col_perp2 = _get_valid_next_coords(current_row, current_col, dr_perp2, dc_perp2)\n",
        "    next_state_perp2 = coords_to_state(next_row_perp2, next_col_perp2)\n",
        "    possible_transitions.append((0.1, next_state_perp2))\n",
        "\n",
        "    # Combine probabilities for identical next states\n",
        "    # This handles cases where different actions lead to the same 'next_state'\n",
        "    # e.g., hitting a wall with an intended move, and a perpendicular move also hits the same wall or is invalid into the current state.\n",
        "    merged_transitions = {}\n",
        "    for prob, state_idx in possible_transitions:\n",
        "        merged_transitions[state_idx] = merged_transitions.get(state_idx, 0.0) + prob\n",
        "\n",
        "    return [(prob, state_idx) for state_idx, prob in merged_transitions.items()]\n",
        "\n",
        "def get_reward(state):\n",
        "    \"\"\"\n",
        "    Returns the reward associated with a specific state.\n",
        "    \"\"\"\n",
        "    return REWARDS[state]\n",
        "\n",
        "print(\"Functions `get_next_state` and `get_reward` defined.\")\n",
        "\n",
        "# Example Usage:\n",
        "# Test from state 0 (top-left) with action RIGHT\n",
        "# Expected: intended to state 1, perpendicular to 3 (DOWN) or 0 (UP, hit wall)\n",
        "# current_state = 0 # (0,0)\n",
        "# action = ACTIONS['RIGHT'] # 3\n",
        "# transitions = get_next_state(current_state, action)\n",
        "# print(f\"Transitions from state {current_state} with action {ACTIONS_NAMES[action]}: {transitions}\")\n",
        "\n",
        "# Test reward for goal state\n",
        "# print(f\"Reward for GOAL_STATE ({GOAL_STATE}): {get_reward(GOAL_STATE)}\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Functions `get_next_state` and `get_reward` defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "112c0d3e"
      },
      "source": [
        "## Simulate Random Episodes\n",
        "\n",
        "### Subtask:\n",
        "Develop code to simulate several random episodes in the defined grid world. The robot will move through the environment based on random actions, and its path will be recorded for later review.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cb93faa"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires simulating random episodes in the grid world, recording the path, and printing a summary for each episode. I will implement a Python code block that follows all the instructions given, including initializing `episode_history`, looping through episodes and steps, selecting random actions and next states based on probabilities, and storing the transitions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2feaaa3",
        "outputId": "13099c09-ded3-4404-c8fb-dca8fd48cab5"
      },
      "source": [
        "import random\n",
        "\n",
        "# 1. Initialize an empty list to store episode history\n",
        "episode_history = []\n",
        "\n",
        "# Parameters for simulation\n",
        "NUM_EPISODES = 10\n",
        "MAX_STEPS_PER_EPISODE = 20\n",
        "\n",
        "print(f\"Simulating {NUM_EPISODES} episodes...\")\n",
        "\n",
        "# 2. Loop to simulate a fixed number of episodes\n",
        "for episode_num in range(NUM_EPISODES):\n",
        "    episode_transitions = []\n",
        "\n",
        "    # 3. Randomly choose a starting state, ensuring it's not a goal or danger state\n",
        "    while True:\n",
        "        start_state = random.randint(0, NUM_STATES - 1)\n",
        "        if start_state != GOAL_STATE and start_state not in DANGER_STATES:\n",
        "            break\n",
        "\n",
        "    current_state = start_state\n",
        "    done = False\n",
        "    steps_taken = 0\n",
        "\n",
        "    # 5. Inner loop to simulate steps within an episode\n",
        "    while not done and steps_taken < MAX_STEPS_PER_EPISODE:\n",
        "        # 6a. Select a random action\n",
        "        action_idx = random.choice(list(ACTIONS.values()))\n",
        "        action_name = ACTIONS_NAMES[action_idx]\n",
        "\n",
        "        # 6b. Use get_next_state to get possible next states and their probabilities\n",
        "        possible_transitions = get_next_state(current_state, action_idx)\n",
        "\n",
        "        # 6c. Randomly select one next_state based on probabilities\n",
        "        # Unzip the list of (probability, state) tuples into two separate lists\n",
        "        probabilities, states = zip(*possible_transitions)\n",
        "        # Use random.choices to select a state based on the provided probabilities\n",
        "        next_state = random.choices(states, weights=probabilities, k=1)[0]\n",
        "\n",
        "        # 6d. Get the reward for the next_state\n",
        "        reward = get_reward(next_state)\n",
        "\n",
        "        # 6e. Append the tuple to episode_transitions\n",
        "        episode_transitions.append((current_state, action_name, reward, next_state))\n",
        "\n",
        "        # 6f. Update current_state to next_state\n",
        "        current_state = next_state\n",
        "\n",
        "        # 6g. Check if the current_state is a terminal state\n",
        "        if current_state == GOAL_STATE or current_state in DANGER_STATES:\n",
        "            done = True\n",
        "\n",
        "        steps_taken += 1\n",
        "\n",
        "    episode_history.extend(episode_transitions) # Add current episode's transitions to overall history\n",
        "\n",
        "    # 7. Print a summary of the episode\n",
        "    print(f\"Episode {episode_num + 1} finished. Total steps: {steps_taken}. Final state: {current_state} ({'Goal' if current_state == GOAL_STATE else 'Danger' if current_state in DANGER_STATES else 'In progress'}) from start state: {start_state}\")\n",
        "\n",
        "print(\"Simulation complete. Episode history stored.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulating 10 episodes...\n",
            "Episode 1 finished. Total steps: 12. Final state: 2 (Goal) from start state: 8\n",
            "Episode 2 finished. Total steps: 4. Final state: 4 (Danger) from start state: 1\n",
            "Episode 3 finished. Total steps: 1. Final state: 4 (Danger) from start state: 7\n",
            "Episode 4 finished. Total steps: 8. Final state: 2 (Goal) from start state: 1\n",
            "Episode 5 finished. Total steps: 20. Final state: 6 (In progress) from start state: 0\n",
            "Episode 6 finished. Total steps: 20. Final state: 6 (In progress) from start state: 5\n",
            "Episode 7 finished. Total steps: 20. Final state: 2 (Goal) from start state: 3\n",
            "Episode 8 finished. Total steps: 14. Final state: 4 (Danger) from start state: 3\n",
            "Episode 9 finished. Total steps: 1. Final state: 4 (Danger) from start state: 5\n",
            "Episode 10 finished. Total steps: 12. Final state: 4 (Danger) from start state: 7\n",
            "Simulation complete. Episode history stored.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99f272e5"
      },
      "source": [
        "## Visualize Robot Movement (Optional)\n",
        "\n",
        "### Subtask:\n",
        "Create a simple visualization (e.g., using print statements or a text-based grid representation) to show the robot's movement through the grid during a simulated episode. This will help in understanding the environment's dynamics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c05d3ce"
      },
      "source": [
        "## Visualize Robot Movement (Optional)\n",
        "\n",
        "### Subtask:\n",
        "Create a simple visualization (e.g., using print statements or a text-based grid representation) to show the robot's movement through the grid during a simulated episode. This will help in understanding the environment's dynamics.\n",
        "\n",
        "#### Instructions\n",
        "1. Define a function, say `print_grid_state(current_state, action_name, next_state)`, that takes the current state, the action taken, and the next state as input.\n",
        "2. Inside this function, create a 3x3 grid representation (e.g., a list of lists or a 2D array).\n",
        "3. Mark the goal state ('G'), danger states ('D'), and the robot's current position ('R') on the grid. If the current position is also a goal or danger state, you can prioritize 'R' or indicate both (e.g., 'RG' or 'RD').\n",
        "4. Print the grid, along with information about the current state, action, and next state.\n",
        "5. Select one or more episodes from the `episode_history` (e.g., the first episode or an episode that reached the goal/danger state) for visualization.\n",
        "6. Iterate through the transitions of the selected episode(s) and call the `print_grid_state` function for each transition to visualize the robot's movement step by step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c332e96c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires implementing a function `print_grid_state` to visualize the robot's movement in the grid and then iterating through selected episodes from `episode_history` to display the steps. This will be done in a code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35a742c4",
        "outputId": "fb7733cc-915c-4fa7-b1db-b7d2b44388da"
      },
      "source": [
        "def print_grid_state(current_state, action_name, next_state):\n",
        "    \"\"\"\n",
        "    Prints a 3x3 grid representation of the MDP, marking goal, danger, and robot's position.\n",
        "    \"\"\"\n",
        "    grid = [['.' for _ in range(GRID_SIZE[1])] for _ in range(GRID_SIZE[0])]\n",
        "\n",
        "    # Mark Goal state\n",
        "    g_row, g_col = state_to_coords(GOAL_STATE)\n",
        "    grid[g_row][g_col] = 'G'\n",
        "\n",
        "    # Mark Danger states\n",
        "    for d_s in DANGER_STATES:\n",
        "        d_row, d_col = state_to_coords(d_s)\n",
        "        if grid[d_row][d_col] == '.': # Avoid overwriting 'G' if by chance goal is also danger\n",
        "            grid[d_row][d_col] = 'D'\n",
        "\n",
        "    # Mark Robot's current position\n",
        "    r_row, r_col = state_to_coords(current_state)\n",
        "    if grid[r_row][r_col] == 'G':\n",
        "        grid[r_row][r_col] = 'RG' # Robot is on Goal\n",
        "    elif grid[r_row][r_col] == 'D':\n",
        "        grid[r_row][r_col] = 'RD' # Robot is on Danger\n",
        "    else:\n",
        "        grid[r_row][r_col] = 'R'\n",
        "\n",
        "    print(f\"\\n--- Step ---\")\n",
        "    print(f\"Current State: {current_state} (R)\")\n",
        "    print(f\"Action Taken: {action_name}\")\n",
        "    print(f\"Next State: {next_state}\")\n",
        "    print(\"Grid:\")\n",
        "    for row in grid:\n",
        "        print(' '.join(row))\n",
        "    print(\"------------\")\n",
        "\n",
        "print(\"Function `print_grid_state` defined.\")\n",
        "\n",
        "# Visualize a specific episode (e.g., the first one)\n",
        "if episode_history:\n",
        "    print(f\"\\nVisualizing the first episode from `episode_history` (Total steps: {len(episode_history)})...\")\n",
        "    # For simplicity, let's visualize the first 'MAX_STEPS_PER_EPISODE' steps or until a terminal state\n",
        "    # if the episode history contains more than one episode's steps, we need to filter.\n",
        "    # Assuming episode_history stores all transitions sequentially.\n",
        "    # Let's take the first complete episode. An episode ends when current_state is a terminal state\n",
        "    # or MAX_STEPS_PER_EPISODE is reached.\n",
        "\n",
        "    # Find the end of the first episode\n",
        "    first_episode_transitions = []\n",
        "    for i, transition in enumerate(episode_history):\n",
        "        first_episode_transitions.append(transition)\n",
        "        # A transition is (current_state, action_name, reward, next_state)\n",
        "        # The next_state is the state reached after the action.\n",
        "        # Check if the next_state is terminal\n",
        "        if transition[3] == GOAL_STATE or transition[3] in DANGER_STATES:\n",
        "            break\n",
        "\n",
        "    for current_s, action_n, reward_val, next_s in first_episode_transitions:\n",
        "        print_grid_state(current_s, action_n, next_s)\n",
        "    print(f\"Visualization of first episode completed.\")\n",
        "else:\n",
        "    print(\"No episode history to visualize.\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function `print_grid_state` defined.\n",
            "\n",
            "Visualizing the first episode from `episode_history` (Total steps: 112)...\n",
            "\n",
            "--- Step ---\n",
            "Current State: 8 (R)\n",
            "Action Taken: RIGHT\n",
            "Next State: 5\n",
            "Grid:\n",
            ". . G\n",
            ". D .\n",
            ". . R\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 5 (R)\n",
            "Action Taken: UP\n",
            "Next State: 5\n",
            "Grid:\n",
            ". . G\n",
            ". D R\n",
            ". . .\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 5 (R)\n",
            "Action Taken: DOWN\n",
            "Next State: 8\n",
            "Grid:\n",
            ". . G\n",
            ". D R\n",
            ". . .\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 8 (R)\n",
            "Action Taken: LEFT\n",
            "Next State: 7\n",
            "Grid:\n",
            ". . G\n",
            ". D .\n",
            ". . R\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 7 (R)\n",
            "Action Taken: LEFT\n",
            "Next State: 6\n",
            "Grid:\n",
            ". . G\n",
            ". D .\n",
            ". R .\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 6 (R)\n",
            "Action Taken: LEFT\n",
            "Next State: 6\n",
            "Grid:\n",
            ". . G\n",
            ". D .\n",
            "R . .\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 6 (R)\n",
            "Action Taken: RIGHT\n",
            "Next State: 7\n",
            "Grid:\n",
            ". . G\n",
            ". D .\n",
            "R . .\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 7 (R)\n",
            "Action Taken: RIGHT\n",
            "Next State: 8\n",
            "Grid:\n",
            ". . G\n",
            ". D .\n",
            ". R .\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 8 (R)\n",
            "Action Taken: UP\n",
            "Next State: 5\n",
            "Grid:\n",
            ". . G\n",
            ". D .\n",
            ". . R\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 5 (R)\n",
            "Action Taken: DOWN\n",
            "Next State: 5\n",
            "Grid:\n",
            ". . G\n",
            ". D R\n",
            ". . .\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 5 (R)\n",
            "Action Taken: DOWN\n",
            "Next State: 5\n",
            "Grid:\n",
            ". . G\n",
            ". D R\n",
            ". . .\n",
            "------------\n",
            "\n",
            "--- Step ---\n",
            "Current State: 5 (R)\n",
            "Action Taken: UP\n",
            "Next State: 2\n",
            "Grid:\n",
            ". . G\n",
            ". D R\n",
            ". . .\n",
            "------------\n",
            "Visualization of first episode completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65fdf53a"
      },
      "source": [
        "## Implement Simple Greedy Policy\n",
        "\n",
        "### Subtask:\n",
        "Create a basic greedy policy that, from any given state, chooses the action that leads to the highest immediate reward. This will be a first step towards solving the MDP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "019140a2"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define the `greedy_policy(state)` function as per the subtask instructions. This function will iterate through all possible actions, use `get_next_state` to find the most probable next state for each action, and then `get_reward` to determine the immediate reward. Finally, it will return the action that leads to the maximum immediate reward. I will also include a print statement to indicate that the function has been defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a19c703"
      },
      "source": [
        "**Reasoning**:\n",
        "I will implement the `greedy_policy` function as described in the subtask instructions, which involves iterating through actions, finding the most probable next state, calculating its reward, and selecting the action that maximizes this immediate reward. I will also add a print statement to confirm the function definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59b0dd7c",
        "outputId": "4fc82bf8-d080-4377-b1ab-263f12836ea2"
      },
      "source": [
        "def greedy_policy(state):\n",
        "    \"\"\"\n",
        "    A simple greedy policy that chooses the action leading to the highest immediate reward.\n",
        "    It considers only the most probable next state (80% intended move).\n",
        "    \"\"\"\n",
        "    max_immediate_reward = -float('inf')\n",
        "    best_action_idx = None\n",
        "\n",
        "    for action_idx in ACTIONS.values():\n",
        "        # Get possible next states and their probabilities for the current action\n",
        "        possible_transitions = get_next_state(state, action_idx)\n",
        "\n",
        "        # Find the most probable next state (assuming 80% intended move)\n",
        "        # The `get_next_state` function returns tuples of (probability, state_idx).\n",
        "        # We need to find the state with the highest probability.\n",
        "        most_probable_next_state = None\n",
        "        highest_prob = -1.0\n",
        "\n",
        "        for prob, next_s in possible_transitions:\n",
        "            if prob > highest_prob:\n",
        "                highest_prob = prob\n",
        "                most_probable_next_state = next_s\n",
        "\n",
        "        # If for some reason no state was found (shouldn't happen with our current logic),\n",
        "        # or if the current state is terminal, handle appropriately. For non-terminal states,\n",
        "        # a most probable next state will always exist.\n",
        "        if most_probable_next_state is not None:\n",
        "            # Get the reward for this most probable next state\n",
        "            current_reward = get_reward(most_probable_next_state)\n",
        "\n",
        "            # Update best action if this one yields a higher immediate reward\n",
        "            if current_reward > max_immediate_reward:\n",
        "                max_immediate_reward = current_reward\n",
        "                best_action_idx = action_idx\n",
        "\n",
        "    # If all actions lead to equally low or zero reward, or no next state could be determined,\n",
        "    # choose a random action as a fallback, or the first one.\n",
        "    if best_action_idx is None: # This could happen if all transitions lead to terminal states with no reward, or if the state itself is terminal.\n",
        "        best_action_idx = random.choice(list(ACTIONS.values()))\n",
        "\n",
        "    return best_action_idx\n",
        "\n",
        "print(\"Function `greedy_policy` defined.\")\n",
        "\n",
        "# Example Usage:\n",
        "# Test the greedy policy from an arbitrary state, e.g., state 0\n",
        "# test_state = 0\n",
        "# recommended_action_idx = greedy_policy(test_state)\n",
        "# print(f\"Greedy policy from state {test_state} recommends action: {ACTIONS_NAMES[recommended_action_idx]}\")\n",
        "\n",
        "# Test from a state adjacent to goal to see if it moves towards goal\n",
        "# test_state_near_goal = 5 # (1,2)\n",
        "# recommended_action_idx_near_goal = greedy_policy(test_state_near_goal)\n",
        "# print(f\"Greedy policy from state {test_state_near_goal} (near goal) recommends action: {ACTIONS_NAMES[recommended_action_idx_near_goal]}\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function `greedy_policy` defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98fc40bb"
      },
      "source": [
        "## Evaluate Greedy Policy\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the implemented greedy policy. Simulate several episodes where the robot follows this policy, record its path, total reward, and whether it reached a terminal state (goal or danger). Summarize the findings to understand the policy's effectiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2564b47"
      },
      "source": [
        "## Evaluate Greedy Policy\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the implemented greedy policy. Simulate several episodes where the robot follows this policy, record its path, total reward, and whether it reached a terminal state (goal or danger). Summarize the findings to understand the policy's effectiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01750cad"
      },
      "source": [
        "## Evaluate Greedy Policy\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the implemented greedy policy. Simulate several episodes where the robot follows this policy, record its path, total reward, and whether it reached a terminal state (goal or danger). Summarize the findings to understand the policy's effectiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6c5fec8"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires simulating episodes using the greedy policy, recording the path, total reward, and terminal state for each episode. I will implement a Python code block that iterates through a specified number of episodes and steps, using the `greedy_policy` function to choose actions and then `get_next_state` and `get_reward` to update the environment. A summary of each episode will be printed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cec763b",
        "outputId": "3369190e-3274-4846-b28d-c63e4ddadc77"
      },
      "source": [
        "greedy_episode_history = []\n",
        "NUM_GREEDY_EPISODES = 10\n",
        "MAX_GREEDY_STEPS = 50 # Allow more steps to see if it reaches goal/danger\n",
        "\n",
        "print(f\"Simulating {NUM_GREEDY_EPISODES} episodes using the greedy policy...\")\n",
        "\n",
        "for episode_num in range(NUM_GREEDY_EPISODES):\n",
        "    episode_transitions = []\n",
        "    total_reward = 0\n",
        "\n",
        "    # Randomly choose a starting state, ensuring it's not a goal or danger state\n",
        "    while True:\n",
        "        start_state = random.randint(0, NUM_STATES - 1)\n",
        "        if start_state != GOAL_STATE and start_state not in DANGER_STATES:\n",
        "            break\n",
        "\n",
        "    current_state = start_state\n",
        "    done = False\n",
        "    steps_taken = 0\n",
        "\n",
        "    while not done and steps_taken < MAX_GREEDY_STEPS:\n",
        "        # Choose action based on the greedy policy\n",
        "        action_idx = greedy_policy(current_state)\n",
        "        action_name = ACTIONS_NAMES[action_idx]\n",
        "\n",
        "        # Get possible next states and their probabilities\n",
        "        possible_transitions = get_next_state(current_state, action_idx)\n",
        "\n",
        "        # Randomly select one next_state based on probabilities\n",
        "        probabilities, states = zip(*possible_transitions)\n",
        "        next_state = random.choices(states, weights=probabilities, k=1)[0]\n",
        "\n",
        "        # Get the reward for the next_state\n",
        "        reward = get_reward(next_state)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Append the transition to history\n",
        "        episode_transitions.append((current_state, action_name, reward, next_state))\n",
        "\n",
        "        # Update current_state\n",
        "        current_state = next_state\n",
        "\n",
        "        # Check for terminal states\n",
        "        if current_state == GOAL_STATE or current_state in DANGER_STATES:\n",
        "            done = True\n",
        "\n",
        "        steps_taken += 1\n",
        "\n",
        "    greedy_episode_history.extend(episode_transitions)\n",
        "\n",
        "    final_state_type = 'In progress'\n",
        "    if current_state == GOAL_STATE:\n",
        "        final_state_type = 'Goal'\n",
        "    elif current_state in DANGER_STATES:\n",
        "        final_state_type = 'Danger'\n",
        "\n",
        "    print(f\"Episode {episode_num + 1} (Greedy) finished. Steps: {steps_taken}. Total Reward: {total_reward:.2f}. Final state: {current_state} ({final_state_type}) from start state: {start_state}\")\n",
        "\n",
        "print(\"Greedy policy simulation complete. History stored in `greedy_episode_history`.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulating 10 episodes using the greedy policy...\n",
            "Episode 1 (Greedy) finished. Steps: 2. Total Reward: 10.00. Final state: 2 (Goal) from start state: 8\n",
            "Episode 2 (Greedy) finished. Steps: 1. Total Reward: 10.00. Final state: 2 (Goal) from start state: 1\n",
            "Episode 3 (Greedy) finished. Steps: 1. Total Reward: 10.00. Final state: 2 (Goal) from start state: 5\n",
            "Episode 4 (Greedy) finished. Steps: 2. Total Reward: 10.00. Final state: 2 (Goal) from start state: 8\n",
            "Episode 5 (Greedy) finished. Steps: 1. Total Reward: 10.00. Final state: 2 (Goal) from start state: 5\n",
            "Episode 6 (Greedy) finished. Steps: 12. Total Reward: 10.00. Final state: 2 (Goal) from start state: 7\n",
            "Episode 7 (Greedy) finished. Steps: 5. Total Reward: 10.00. Final state: 2 (Goal) from start state: 7\n",
            "Episode 8 (Greedy) finished. Steps: 3. Total Reward: 10.00. Final state: 2 (Goal) from start state: 8\n",
            "Episode 9 (Greedy) finished. Steps: 1. Total Reward: 10.00. Final state: 2 (Goal) from start state: 5\n",
            "Episode 10 (Greedy) finished. Steps: 1. Total Reward: -5.00. Final state: 4 (Danger) from start state: 1\n",
            "Greedy policy simulation complete. History stored in `greedy_episode_history`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b34d73eb"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the implemented MDP, the simulated episodes, and the performance of the simple greedy policy, discussing key findings and potential next steps for policy improvement.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6f1f60a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "*   **What is the summary of the implemented MDP?**\n",
        "    The implemented MDP is a 3x3 grid world with 9 states. Actions include 'UP', 'DOWN', 'LEFT', and 'RIGHT'. The goal state is (0,2) (state 2) with a reward of +10, and a danger state is (1,1) (state 4) with a reward of -5. All other states have a reward of 0. The discount factor is 0.9. Transition dynamics are probabilistic: 80% chance of intended movement and 10% chance for each of the two perpendicular movements, with boundary conditions causing the agent to stay in the current cell if it moves out of bounds.\n",
        "\n",
        "*   **What is the summary of the simulated episodes?**\n",
        "    Initially, 10 random episodes were simulated, with a maximum of 20 steps per episode. These episodes demonstrated varied outcomes: some reached the goal, some hit danger states, and others terminated by reaching the maximum step limit without hitting a terminal state. Later, 10 episodes were simulated using a simple greedy policy, with a maximum of 50 steps per episode.\n",
        "\n",
        "*   **What is the performance of the simple greedy policy?**\n",
        "    The simple greedy policy performed reasonably well. Out of 10 simulated episodes, 9 successfully reached the Goal state (state 2) with a total reward of 10.00 each. One episode ended in a Danger state (state 4) with a total reward of -5.00. The number of steps taken to reach the goal varied from 1 to 12 steps.\n",
        "\n",
        "*   **What are the key findings?**\n",
        "    The key findings indicate that the simple greedy policy, which prioritizes immediate highest reward, is generally effective in guiding the agent towards the goal state in this probabilistic environment, despite occasionally leading to danger due to the random components of state transitions.\n",
        "\n",
        "*   **What are potential next steps for policy improvement?**\n",
        "    Potential next steps for policy improvement include implementing more sophisticated policy learning algorithms such as Value Iteration or Q-learning, which consider long-term discounted rewards rather than just immediate rewards, and potentially incorporating an exploration-exploitation strategy.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The MDP consists of a 3x3 grid (9 states), 4 actions (UP, DOWN, LEFT, RIGHT), a goal state (state 2) with a reward of +10, and a danger state (state 4) with a reward of -5. The discount factor is 0.9.\n",
        "*   Transition probabilities involve an 80% chance for the intended action and 10% for each of the two perpendicular actions, with boundary conditions handling out-of-bounds movements by keeping the agent in its current state.\n",
        "*   Initial random simulations showed that agents can reach goal states, danger states, or exhaust the maximum step limit (20 steps) without reaching a terminal state.\n",
        "*   The implemented simple greedy policy, which selects actions based on the immediate reward of the most probable next state, was evaluated over 10 episodes.\n",
        "*   The greedy policy led to reaching the Goal state (state 2) in 9 out of 10 episodes, achieving a total reward of 10.00 for each successful episode.\n",
        "*   One episode under the greedy policy resulted in reaching the Danger state (state 4), yielding a total reward of -5.00.\n",
        "*   The number of steps taken to reach a terminal state by the greedy policy varied, ranging from 1 to 12 steps for successful episodes.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The simple greedy policy, while effective in many cases, is not optimal due to the probabilistic transitions and its sole focus on immediate rewards. It fails to account for potential future negative rewards or longer paths that might lead to a larger cumulative discounted reward.\n",
        "*   To improve policy performance and robustness, implement and evaluate more advanced reinforcement learning algorithms such as Value Iteration or Q-learning. These methods consider the long-term expected discounted reward, which is crucial in stochastic environments, potentially leading to a policy that avoids danger states more reliably and finds more efficient paths to the goal.\n"
      ]
    }
  ]
}